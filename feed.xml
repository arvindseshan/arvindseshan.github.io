<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://arvindseshan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://arvindseshan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-28T15:00:28+00:00</updated><id>https://arvindseshan.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Image Inpainting</title><link href="https://arvindseshan.github.io/projects/image-inpainting/" rel="alternate" type="text/html" title="Image Inpainting"/><published>2024-12-26T11:46:00+00:00</published><updated>2024-12-26T11:46:00+00:00</updated><id>https://arvindseshan.github.io/projects/image-inpainting</id><content type="html" xml:base="https://arvindseshan.github.io/projects/image-inpainting/"><![CDATA[<p>For this project, I designed and implemented process for inpainting regions in images by interpolating structure (image gradients) using a Poisson solver and color values via PDE’s (anisotropic diffusion) in <code>C++</code>.</p> <p>With the recent popularity of generative AI (diffusion models) for a variety of visual tasks including inpainting, the goal was to take a deeper dive into classical methods for image inpainting.</p> <h2 id="poisson-solver">Poisson Solver</h2> <p>The first step for this project was to implement a Poisson solver. Below, you can see a test case that runs the Poisson solver for seamless compositing given a background (water), foreground (bear), and mask. The blended result is shown in the bottom right image.</p> <div class="social"> <a class="venobox" data-gall="myGallery" href="../../../assets/img/waterpool.png"><img style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/waterpool.png"/></a> <a class="venobox" data-gall="myGallery" href="../../../assets/img/mask.png"><img style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/mask.png"/></a><br/> <a class="venobox" data-gall="myGallery" href="../../../assets/img/bear.png"><img style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/bear.png"/></a> <a class="venobox" data-gall="myGallery" href="../../../assets/img/poissonTest.png"><img style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/poissonTest.png"/></a> </div> <h2 id="laplacian">Laplacian</h2> <p>Next, I had to compute the Laplacian of an image. The test case below runs the code to compute the Laplacian by convolving with the appropriate kernel. Drag the slider on the image to reveal the original image and its corresponding Laplacian. The Laplacian is a second derivative and should highlight regions of the image with drastic changes in pixel intensity (i.e. edges).</p> <div class="social"> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/beach-480.webp 480w,/assets/img/beach-800.webp 800w,/assets/img/beach-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/beach.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/applyLaplacianTest-480.webp 480w,/assets/img/applyLaplacianTest-800.webp 800w,/assets/img/applyLaplacianTest-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/applyLaplacianTest.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <h2 id="structure-tensor">Structure Tensor</h2> <p>I also compute the structure tensor of an image. The test case below runs the code to compute the structure tensor while ignoring pixels within the masked area. Drag the slider on the image to reveal the original image with the mask and its corresponding structure tensor. The red and blue coloring indicates a change in intensity in primarily the horizontal and vertical directions respectively. Also, note how the tensor output is correctly blacked out in the masked regions.</p> <div class="social"> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/street_with_mask-480.webp 480w,/assets/img/street_with_mask-800.webp 800w,/assets/img/street_with_mask-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/street_with_mask.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tensorMaskTest-480.webp 480w,/assets/img/tensorMaskTest-800.webp 800w,/assets/img/tensorMaskTest-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/tensorMaskTest.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <h2 id="structure-interpolation">Structure Interpolation</h2> <p>With these components, the next step is to interpolate the structure of the masked regions. To do this, I first compute the structure tensor outside the masked area. Then, I run the Poisson solver with the initial structure tensor as the background and a black image as the foreground. The test case below runs the code to interpolate the structure tensor within the masked area of the image. The image on the left is the full structure tensor output and the image on the right depicts just the interpolated parts in the masked region.</p> <div class="social"> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/InterpolateStructureTestStreet-480.webp 480w,/assets/img/InterpolateStructureTestStreet-800.webp 800w,/assets/img/InterpolateStructureTestStreet-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/InterpolateStructureTestStreet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/InterpolateStructureTestMaskStreet-480.webp 480w,/assets/img/InterpolateStructureTestMaskStreet-800.webp 800w,/assets/img/InterpolateStructureTestMaskStreet-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/InterpolateStructureTestMaskStreet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> <h2 id="color-interpolation">Color Interpolation</h2> <p>Now that I have an interpolated structure tensor, I can use it to interpolate color values within the masked area. To do this, I iterate over the image and perform iterative image inpainting updates. I interleave these updates with a pass of anisotropic diffusion every third iteration. I only update pixels within the masked region. Below are the results of the algorithm on a few sample images.</p> <div class="social"> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/street_with_mask-480.webp 480w,/assets/img/street_with_mask-800.webp 800w,/assets/img/street_with_mask-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/street_with_mask.png" class="img-fluid rounded z-depth-1" width="300" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AnisotropicDiffusionTestStreet-480.webp 480w,/assets/img/AnisotropicDiffusionTestStreet-800.webp 800w,/assets/img/AnisotropicDiffusionTestStreet-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/AnisotropicDiffusionTestStreet.png" class="img-fluid rounded z-depth-1" width="300" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inpaintingbeach-480.webp 480w,/assets/img/inpaintingbeach-800.webp 800w,/assets/img/inpaintingbeach-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/inpaintingbeach.png" class="img-fluid rounded z-depth-1" width="300" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inpaintingbeach2-480.webp 480w,/assets/img/inpaintingbeach2-800.webp 800w,/assets/img/inpaintingbeach2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/inpaintingbeach2.png" class="img-fluid rounded z-depth-1" width="300" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> </div> ]]></content><author><name></name></author><category term="class-projects"/><category term="C/C++"/><category term="visual-computing"/><summary type="html"><![CDATA[Designed and implemented process for inpainting regions in images by interpolating structure (image gradients) using a Poisson solver and color values via PDE’s (anisotropic diffusion) in C++.]]></summary></entry><entry><title type="html">UNIX-like Operating System</title><link href="https://arvindseshan.github.io/projects/os/" rel="alternate" type="text/html" title="UNIX-like Operating System"/><published>2024-12-25T11:40:00+00:00</published><updated>2024-12-25T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/os</id><content type="html" xml:base="https://arvindseshan.github.io/projects/os/"><![CDATA[<p>Implemented several key parts of virtual memory system, network protocol stack, system call interfaces, and file system in <code class="language-plaintext highlighter-rouge">C</code>.</p>]]></content><author><name></name></author><category term="class-projects"/><category term="C/C++"/><category term="systems"/><summary type="html"><![CDATA[Implemented several key parts of virtual memory system, network protocol stack, system call interfaces, and file system in C.]]></summary></entry><entry><title type="html">RISC-V Processor Optimized for Neural Network Inference</title><link href="https://arvindseshan.github.io/projects/Processor/" rel="alternate" type="text/html" title="RISC-V Processor Optimized for Neural Network Inference"/><published>2024-12-24T11:40:00+00:00</published><updated>2024-12-24T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/Processor</id><content type="html" xml:base="https://arvindseshan.github.io/projects/Processor/"><![CDATA[<p>Built CPU with 4-stage pipeline, bypassing, and custom instruction set extensions for inference. Implemented loop unrolling, and function inlining code optimizations.</p>]]></content><author><name></name></author><category term="class-projects"/><category term="C/C++"/><category term="systems"/><category term="machine-learning"/><summary type="html"><![CDATA[Built CPU with 4-stage pipeline, bypassing, and custom instruction set extensions for inference. Implemented loop unrolling, and function inlining code optimizations.]]></summary></entry><entry><title type="html">Iterative Neural Network Based Approach to Automated IFT-20 Sensory Neuron Identification in Caenorhabditis elegans</title><link href="https://arvindseshan.github.io/projects/celegans/" rel="alternate" type="text/html" title="Iterative Neural Network Based Approach to Automated IFT-20 Sensory Neuron Identification in Caenorhabditis elegans"/><published>2024-12-23T11:40:00+00:00</published><updated>2024-12-23T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/celegans</id><content type="html" xml:base="https://arvindseshan.github.io/projects/celegans/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">Paper available upon request</code></p> <p>Determining neuronal identity in imaging data is an essential task in neuroscience, facilitating the comparison of neural activity across organisms. Cross-organism comparison, in turn, enables a wide variety of research including whole-brain analysis of functional networks and linking the activity of specific neurons to behavior or environmental stimuli. The recent development of three-dimensional, pan-neuronal imaging with single-cell resolution within <em>Caenorhabditis elegans</em> has brought neuron identification, tracking, and activity monitoring all within reach. The nematode <em>C. elegans</em> is often used as a model organism to study neuronal activity due to factors such as its transparency and well-understood nervous system. The principal barrier to high-accuracy neuron identification is that in adult <em>C. elegans</em>, the position of neuronal cell bodies is not stereotyped. Existing approaches to address this issue use genetically encoded markers as an additional identifying feature. For example, the NeuroPAL strain uses multicolored fluorescent reporters. However, this approach has limited use due to the negative effects of excessive genetic modification. In this study, I propose an alternative neuronal identification technique using only single-color fluorescent images. I designed a novel neural network based classifier that automatically labels sensory neurons using an iterative, landmark-based neuron identification process inspired by the manual annotation procedures that humans employ. This design labels sensory neurons in <em>C. elegans</em> with 91.61% accuracy.</p> <div class="social"> <a class="venobox" data-gall="myGallery1" href="../../../assets/img/65Wormxy.png"><img src="../../../assets/img/65Wormxysmall.png"/></a> <a class="venobox" data-gall="myGallery1" href="../../../assets/img/65Wormxz.png"><img src="../../../assets/img/65Wormxzsmall.png"/></a> <a class="venobox" data-gall="myGallery1" href="../../../assets/img/65Wormyz.png"><img src="../../../assets/img/65Wormyzsmall.png"/></a> <a class="venobox" data-gall="myGallery1" href="../../../assets/img/65Worm.png"><img src="../../../assets/img/65Wormsmall.png"/></a> </div>]]></content><author><name></name></author><category term="papers"/><category term="Python"/><category term="visual-computing"/><category term="machine-learning"/><summary type="html"><![CDATA[Developed custom multistage neural network implemented using PyTorch to classify 22 sensory neurons in the IFT-20 subunit of the C. elegans brain. Paper available upon request]]></summary></entry><entry><title type="html">Using Machine Learning to Augment Dynamic Time Warping Based Signal Classification</title><link href="https://arvindseshan.github.io/projects/dtw/" rel="alternate" type="text/html" title="Using Machine Learning to Augment Dynamic Time Warping Based Signal Classification"/><published>2024-12-22T11:40:00+00:00</published><updated>2024-12-22T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/dtw</id><content type="html" xml:base="https://arvindseshan.github.io/projects/dtw/"><![CDATA[<p><a href="https://arxiv.org/abs/2206.07200">arXiv:2206.07200</a></p> <p>Modern applications such as voice recognition rely on the ability to compare signals to pre-recorded ones to classify them. However, this comparison typically needs to ignore differences due to signal noise, temporal offset, signal magnitude, and other external factors. The Dynamic Time Warping (DTW) algorithm quantifies this similarity by finding corresponding regions between the signals and non-linearly warping one signal by stretching and shrinking it. Unfortunately, searching through all “warps” of a signal to find the best corresponding regions is computationally expensive. The FastDTW algorithm improves performance, but sacrifices accuracy by only considering small signal warps.</p> <p>My goal is to improve the speed of DTW while maintaining high accuracy. My key insight is that in any particular application domain, signals exhibit specific types of variation. For example, the accelerometer signal measured for two different people would differ based on their stride length and weight. My system, called Machine Learning DTW (MLDTW), uses machine learning to learn the types of warps that are common in a particular domain. It then uses the learned model to improve DTW performance by limiting the search of potential warps appropriately. My results show that compared to FastDTW, MLDTW is at least as fast and reduces errors by 60% on average across four different data sets. These improvements will significantly impact a wide variety of applications (e.g. health monitoring) and enable more scalable processing of multivariate, higher frequency, and longer signal recordings.</p> <div class="social"> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/AccelCompare-ML.png"><img src="../../../assets/img/AccelCompare-MLsmall.png"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/dtw_sig2.png"><img src="../../../assets/img/dtw_sig2small.png"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/warpingsinewaves.png"><img src="../../../assets/img/warpingsinewavessmall.png"/></a> </div> <p><br/></p> <div class="social"> <figure> <iframe src="https://www.youtube.com/embed/0jVn__d-3oQ?si=sUGgHM57Ns4RqOFu" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="560" height="315"/> </figure> </div>]]></content><author><name></name></author><category term="papers"/><category term="Python"/><category term="machine-learning"/><summary type="html"><![CDATA[Designed neural network using TensorFlow that predicts likely distortions between signals to eliminate unnecessary computations in Dynamic Time Warping (DTW), a popular signal comparison algorithm. arXiv:2206.07200]]></summary></entry><entry><title type="html">Object Tracking &amp;amp; Gesture Recognition</title><link href="https://arvindseshan.github.io/projects/gesture/" rel="alternate" type="text/html" title="Object Tracking &amp;amp; Gesture Recognition"/><published>2024-12-21T11:40:00+00:00</published><updated>2024-12-21T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/gesture</id><content type="html" xml:base="https://arvindseshan.github.io/projects/gesture/"><![CDATA[<p>I created an interactive game with real-time hand-tracking and neural network-based gesture identification using <code class="language-plaintext highlighter-rouge">OpenCV</code> and <code class="language-plaintext highlighter-rouge">TensorFlow</code> in <code class="language-plaintext highlighter-rouge">Python</code>. The project won 1st place term project.</p> <p>The name of my project is Magic Axolotl Academy. Check out the <a href="https://www.youtube.com/watch?v=ZiF5McRtccM">demo video <i class="fa-brands fa-youtube"></i></a> and the <a href="https://github.com/arvindseshan/Magic-Axolotl-Academy">GitHub repository <i class="fa-brands fa-github"></i></a>.</p> <p>It is a spell casting game where a player uses different hand movements to cast various spells (—, |, u, n, ⚡, and ♥) and move the main character. The main character is Kimchee the Axolotl. The main enemies are smog monsters because the biggest threat to axolotls are urbanization and pollution, threatening their environment. Enemies can be defeated by performing the correct order of spells displayed over the enemy when a line of sight is established between the alxolotl and a monster. The idea for the game is based on the 2016 and 2020 Google Doodle called <a href="https://www.google.com/doodles/halloween-2020">Magic Cat Academy</a>.</p> <div class="social"> <figure> <iframe src="https://www.youtube.com/embed/ZiF5McRtccM?si=Gr1SDeJ9deYJi6zd" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="560" height="315"/> </figure> </div> ]]></content><author><name></name></author><category term="class-projects"/><category term="Python"/><category term="visual-computing"/><category term="machine-learning"/><summary type="html"><![CDATA[Created interactive game with real-time hand-tracking and neural network-based gesture identification using OpenCV and TensorFlow in Python. 1st place term project.]]></summary></entry><entry><title type="html">Enabling High-Accuracy Human Activity Recognition with Fine-Grained Indoor Localization</title><link href="https://arvindseshan.github.io/projects/har/" rel="alternate" type="text/html" title="Enabling High-Accuracy Human Activity Recognition with Fine-Grained Indoor Localization"/><published>2024-12-20T11:40:00+00:00</published><updated>2024-12-20T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/har</id><content type="html" xml:base="https://arvindseshan.github.io/projects/har/"><![CDATA[<p><a href="https://arxiv.org/abs/2108.06838">arXiv:2108.06838</a></p> <p>While computers play an increasingly important role in every aspect of our lives, their inability to understand what tasks users are physically performing makes a wide range of applications, including health monitoring and context-specific assistance, difficult or impossible. With Human Activity Recognition (HAR), applications could track if a patient took his pills and detect the behavioral changes associated with diseases such as Alzheimer’s. Current systems for HAR require diverse sensors (e.g., cameras, microphones, proximity sensors, and accelerometers) placed throughout the environment to provide detailed observations needed for high-accuracy HAR. The difficulty of instrumenting an environment with these sensors makes this approach impractical. This project considers whether recent advances in indoor localization (Wi-Fi Round Trip Time) enable high-accuracy HAR using only a smartphone. My design, called Location-Enhanced HAR (LEHAR), uses machine learning to combine acceleration, audio, and location data to detect common human activities. A LEHAR prototype, designed to recognize a dozen common activities conducted in a typical household, achieved an F1-score of 0.965. In contrast, existing approaches, which use only acceleration or audio data, obtained F1-scores of 0.660 and 0.865, respectively, on the same activities. In addition, the F1-score of existing designs dropped significantly as more activities were added for recognition, while LEHAR was able to maintain high accuracy. The results show that using a combination of acceleration, audio, and Wi-Fi Round Trip Time localization can enable a highly accurate and easily deployable HAR system.</p> <div class="social"> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/SoftwareDesign.png"><img height="200px" style=" border-radius: 2%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/SoftwareDesign.png"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/HardwareDesignFull.png"><img height="200px" style=" border-radius: 2%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/HardwareDesignFull.png"/></a> </div>]]></content><author><name></name></author><category term="papers"/><category term="Python"/><category term="Java"/><category term="machine-learning"/><summary type="html"><![CDATA[Used multilateration with fine-grained distance estimates from Wi-Fi Round Trip Time to perform high-accuracy indoor localization. Combined location, accelerometer, and audio data collected from a custom-made Android application programmed in Java to train a neural network that performs activity recognition of various household tasks. arXiv:2108.06838]]></summary></entry><entry><title type="html">(ALTo) Ad Hoc High-Accuracy Touch Interaction Using Acoustic Localization</title><link href="https://arvindseshan.github.io/projects/alto/" rel="alternate" type="text/html" title="(ALTo) Ad Hoc High-Accuracy Touch Interaction Using Acoustic Localization"/><published>2024-12-19T11:40:00+00:00</published><updated>2024-12-19T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/alto</id><content type="html" xml:base="https://arvindseshan.github.io/projects/alto/"><![CDATA[<p><a href="https://arxiv.org/abs/2108.06837">arXiv:2108.06837</a></p> <p>While computers play an increasingly important role in every aspect of our lives, their inability to understand what tasks users are physically performing makes a wide range of applications, including health monitoring and context-specific assistance, difficult or impossible. With Human Activity Recognition (HAR), applications could track if a patient took his pills and detect the behavioral changes associated with diseases such as Alzheimer’s. Current systems for HAR require diverse sensors (e.g., cameras, microphones, proximity sensors, and accelerometers) placed throughout the environment to provide detailed observations needed for high-accuracy HAR. The difficulty of instrumenting an environment with these sensors makes this approach impractical. This project considers whether recent advances in indoor localization (Wi-Fi Round Trip Time) enable high-accuracy HAR using only a smartphone. My design, called Location-Enhanced HAR (LEHAR), uses machine learning to combine acceleration, audio, and location data to detect common human activities. A LEHAR prototype, designed to recognize a dozen common activities conducted in a typical household, achieved an F1-score of 0.965. In contrast, existing approaches, which use only acceleration or audio data, obtained F1-scores of 0.660 and 0.865, respectively, on the same activities. In addition, the F1-score of existing designs dropped significantly as more activities were added for recognition, while LEHAR was able to maintain high accuracy. The results show that using a combination of acceleration, audio, and Wi-Fi Round Trip Time localization can enable a highly accurate and easily deployable HAR system.</p> <div class="social"> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/board2D.png"><img src="../../../assets/img/board2Dsmall.png"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/2-DTest2.png"><img src="../../../assets/img/2-DTest2small.png"/></a> </div>]]></content><author><name></name></author><category term="papers"/><category term="Python"/><summary type="html"><![CDATA[Designed low-cost tool programmed using Python to increase the usable interactive touch surface area of devices. Implemented system using pairs of piezoelectric microphones and time difference of arrival (TDOA) based multilateration to determine the origin of a tap on a surface. arXiv:2108.06837]]></summary></entry><entry><title type="html">Robots for LEGO MINDSTORMS App</title><link href="https://arvindseshan.github.io/projects/robotsapp/" rel="alternate" type="text/html" title="Robots for LEGO MINDSTORMS App"/><published>2024-12-18T11:40:00+00:00</published><updated>2024-12-18T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/robotsapp</id><content type="html" xml:base="https://arvindseshan.github.io/projects/robotsapp/"><![CDATA[<p>Between 2020 and 2022, I collaborated with my brother, Sanjay Seshan, to design three LEGO robotics models for the newly launched MINDSTORMS product.</p> <p>In May 2021, the LEGO group added the first of our three LEGO models to the new 51515 MINDSTORMS Robot Inventor Set. Melody Maker was designed to be a quick build to capture curiosity and inspire young users to explore the basic features of Robot Inventor. The robot drives over the colored bricks to play musical notes. A hidden mode also allows the user to color the color sensor in the model to drive over a strip of paper.</p> <p>In June 2021, the LEGO Group added our Print and Scan robot to the Community section of the Robot Inventor App. My brother and I had been known for creating a series of printing machines over the years. This particular model was unusual because it used the parts only in one set. This printer could print letters.</p> <p>In August 2022, the LEGO released our third model, Color Catcher, was added to the official LEGO MINDSTORMS software. The model featured a newly added multi-robot functionality in the MINDSTORMS software. Color Cather was a fun, fast-paced two-player game that requires two MINDSTOMRS sets to play. Players race to “catch” the randomly picked color that is displayed on the hub. We hope you enjoy playing this game.</p> <p>To download build instructions and code for any of these models, install the newest MINDSTORMS App and go to the Community section.</p> <div class="social"> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/MelodyMakerImage.png"><img height="200px" style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/MelodyMakerImage.png"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/PlotterImage.png"><img height="200px" style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/PlotterImage.png"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/ColorCatcher.png"><img height="200px" style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/ColorCatcher.png"/></a> </div> <div class="social"> <figure> <iframe src="https://www.youtube.com/embed/d9SodrA0tjw?si=WLfbDmxHDUZZ-F79" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="560" height="315"/> </figure> </div>]]></content><author><name></name></author><category term="robotics"/><summary type="html"><![CDATA[Designed 3 robots for official MINDSTORMS App, incorporating end-user needs and product design guidelines.]]></summary></entry><entry><title type="html">LEGO Robotics Games and Other Projects</title><link href="https://arvindseshan.github.io/projects/robotsproj/" rel="alternate" type="text/html" title="LEGO Robotics Games and Other Projects"/><published>2024-12-17T11:40:00+00:00</published><updated>2024-12-17T11:40:00+00:00</updated><id>https://arvindseshan.github.io/projects/robotsproj</id><content type="html" xml:base="https://arvindseshan.github.io/projects/robotsproj/"><![CDATA[<p>For the last seven years, I have been creating and sharing interactive LEGO robotics models. Many of these robots have been featured on LEGO and LEGO Education social media sites, and LEGO events such as LEGOWORLD Copenhagen.</p> <p>Many of my designs are based on parts from just one kit to ensure that others can build them. Tic-Tac-Toe and Rock-Paper-Scissors are two fun games I created where a human player plays against the LEGO robot.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/9Qm_rJGbT8U?si=1QBkTvv1lKpx497K" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="400" height="225"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/MwoE_gScDd8?si=pY9l9h604jv-AKQg" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="400" height="225"/> </figure> </div> </div> <p>Right before I left for MIT, I built this Firefly robot as an interactive pet. The Money Reader model was designed to use the machine learning capabilities of the newly released MINDSTORMS set.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/urtHeseOfl0?si=RVE6PNBnmniCW1Zl" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="400" height="225"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/YHD4KboNIJE?si=PONpHU-tl7qoOEhO" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="400" height="225"/> </figure> </div> </div> <p>Connect 4 allows a player to play against a SPIKE Prime robot. LEGO Education featured this robot in all its social media channels. The SPIKE Prime, which was coded using Python, keeps track of all moves and optimizes its move each round.</p> <p>Another SPIKE Prime robot that I created in collaboration with Sanjay featured a SPIKE Prime robot that prints images. Images get converted to a bitmap on a computer. The Python program on the SPIKE Prime reads the bitmap and prints it out. The printhead moves to the appropriate pixel and draws a dot.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/MaaT5R_IRXs?si=YHuBTVJwqAE8XJ-X" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="400" height="225"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.facebook.com/plugins/video.php?height=314&amp;href=https%3A%2F%2Fwww.facebook.com%2FLEGOeducationOfficial%2Fvideos%2F2707195376264589%2F&amp;show_text=false&amp;width=560&amp;t=0" class="rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="400" height="225"/> </figure> </div> </div> <p>You can see many other projects on my <a href="https://www.youtube.com/@SeshanBrothers/videos">YouTube Channel <i class="fa-brands fa-youtube"></i></a>.</p> <div class="social"> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/firefly.JPG"><img height="300px" style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/firefly.JPG"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/rps.JPG"><img height="300px" style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/rps.JPG"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/connect.JPG"><img height="300px" style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/connect.JPG"/></a> <a class="venobox" data-gall="myGallery2" href="../../../assets/img/tictactoe.JPG"><img height="300px" style=" border-radius: 5%;vertical-align:middle;margin:2px 2px" src="../../../assets/img/tictactoe.JPG"/></a> </div>]]></content><author><name></name></author><category term="robotics"/><summary type="html"><![CDATA[For the last seven years, I have been creating and sharing interactive LEGO robotics models. Many of these robots have been featured on LEGO and LEGO Education social media sites, and LEGO events such as LEGOWORLD Copenhagen.]]></summary></entry></feed>